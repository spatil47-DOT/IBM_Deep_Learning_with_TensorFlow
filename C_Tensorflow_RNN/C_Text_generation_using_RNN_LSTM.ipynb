{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C_Text_generation_using_RNN/LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtZqa5Md0Gx-"
      },
      "source": [
        "\n",
        "# <center> Text generation using RNN/LSTM (Character-level)</center>\n",
        "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
        "The RNN can then be used to generate text character by character that will look like the original training data. \n",
        "\n",
        "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgtOLNr88oMS",
        "outputId": "24f05b79-aae6-4c9a-ce94-fcf8c2b86ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install tensorflow=1.8 and restart Runtime before executiing subsequent cells\n",
        "!pip install tensorflow==1.8.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1MB 63kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.33.2)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (1.0.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.4.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=1c4a96a011217096b91e58d4457677c2f505bca742a93e39f3d216c60aa67652\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2_AC4HA0FpI",
        "outputId": "79203249-db90-4690-bbed-e38f95a1fdbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import codecs\n",
        "import os\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBgOKzkD0Ybg"
      },
      "source": [
        "### Data loader\n",
        "The following cell is a class that help to read data from input file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSGYTeni0Y0o"
      },
      "source": [
        "class TextLoader():\n",
        "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "        self.encoding = encoding\n",
        "\n",
        "        input_file = os.path.join(data_dir, \"input.txt\")\n",
        "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
        "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
        "\n",
        "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
        "            print(\"reading text file\")\n",
        "            self.preprocess(input_file, vocab_file, tensor_file)\n",
        "        else:\n",
        "            print(\"loading preprocessed files\")\n",
        "            self.load_preprocessed(vocab_file, tensor_file)\n",
        "        self.create_batches()\n",
        "        self.reset_batch_pointer()\n",
        "\n",
        "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
        "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
        "            data = f.read()\n",
        "        counter = collections.Counter(data)\n",
        "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
        "        self.chars, _ = zip(*count_pairs)\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "        with open(vocab_file, 'wb') as f:\n",
        "            cPickle.dump(self.chars, f)\n",
        "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
        "        np.save(tensor_file, self.tensor)\n",
        "\n",
        "    def load_preprocessed(self, vocab_file, tensor_file):\n",
        "        with open(vocab_file, 'rb') as f:\n",
        "            self.chars = cPickle.load(f)\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "        self.tensor = np.load(tensor_file)\n",
        "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
        "\n",
        "    def create_batches(self):\n",
        "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
        "\n",
        "        # When the data (tensor) is too small, let's give them a better error message\n",
        "        if self.num_batches==0:\n",
        "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
        "\n",
        "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
        "        xdata = self.tensor\n",
        "        ydata = np.copy(self.tensor)\n",
        "        ydata[:-1] = xdata[1:]\n",
        "        ydata[-1] = xdata[0]\n",
        "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
        "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
        "        self.pointer += 1\n",
        "        return x, y\n",
        "\n",
        "    def reset_batch_pointer(self):\n",
        "        self.pointer = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m9MQ3dc4qJ_"
      },
      "source": [
        "### Parameters\n",
        "#### Batch, number_of_batch, batch_size and seq_length\n",
        "what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n",
        "\n",
        "Lets assume the input is this sentence: '__here is an example__'. Then:\n",
        "- txt_length = 18  \n",
        "- seq_length = 3  \n",
        "- batch_size = 2  \n",
        "- number_of_batchs = 18/3*2 = 3\n",
        "- batch = array (['h','e','r'],['e',' ','i'])\n",
        "- sample Seq = 'her'  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy_hbnok5Naw"
      },
      "source": [
        "seq_length = 50 # RNN sequence length\n",
        "batch_size = 6  # 60 minibatch size, i.e. size of data in each epoch\n",
        "num_epochs = 5 # 50  125,  you should change it to 50 if you want to see a relatively good results\n",
        "learning_rate = 0.002\n",
        "decay_rate = 0.97\n",
        "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
        "num_layers = 2 #number of layers in the RNN"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpIo0gFY5QKd",
        "outputId": "f8d33095-dd37-48d6-dc92-66ceb99fac92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
        "with open('input.txt', 'r') as f:\n",
        "    read_data = f.read()\n",
        "    print (read_data[0:100])\n",
        "f.closed"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-11 05:05:12 URL:https://public.boxcloud.com/d/1/b1!rUD7nU8S-zidDJtijpCnTi2mlrxg7J9UyBaBNuDOoT8vT-jw0MNXRy6OQqBK5hbxcQqvK0X_W5lUhR0pBkJT691cYhvVJLicsrrx-w4Epj-eMv6UqeTS-FZNvsb7DrVIqRGpkPB_lzdfP2pN0xmSDlYjdED_b_mY4qVWz9GghwEb4eXbvBglNyYeCVq9h9H2r5EfMdRxeTzP61Alt4ZUZFvMUHLUKvQTlDbOSxBzAmOMAMblceKY1rVzSoXHh7JpHsjdkGsk8FySuZOqbJOEr4ofRum-a1zHUWdVOGhAxF3OBb9oopOWFF-7pYZ_I1nbKi8gspRqNSPtANVDmzj3G8IUFb4C_uKRsBV0mATEAqqagJ1W04RkiJUAZFnjAQlU5MsrDl1zzi7JbD6aqcfCg11ppX86IILAiTl-ainTwWMFZhP600yC2l9hC1vx_M568EfbZ5cLl1YseyZX38Vs1_hl1deA6MhqYq0KTNzGIVYUc-j_MpX17jYsAtXRQItyqtzfQ04oazpj_OOiialyBpED_67gBuNp5P1CMx4KvhMjTzM4yftBIp9wfONE8beZ4uQhOJcluTb7Pgwldr75mmG9i6k4qskWGn8hmnQlLK3ShNHFTcrMYGftGVmy15YPhT2FZ7C8cupOZwCGNgfTUJVH5xnGYNNnDU5RbJHnDEmxKvKwiv_W40wWeTC8DvEu90A1amAuDQqpGb-PuhSV38kWHvDbWD9ILrzurrfZ_CdwhzT_vPuEAktfMCETr6aSvwLZ_uP8SEDczsMwdrCY6Jnfxj9DZG2rlICgMy5cd9rONLdQ_6wgstxIaRIowlUZLweVoDs5I459DqaRq63eWpAkKYxaqhLNAKunS-c1sa3kR4V5GXefw2kCJvxc1q7wuWGo2o3hqcFOIF2xz8jKRH7DGrGNj_fRwK8RQPzJyTXAKVxOKe9WC86dKRp9HtaOJ3f7eBv7Pe1rZDAkGCmt4lr2hcAtFFvpCPAy6m2w8wgm7ou6_eKJxQi3nX3l4xNuuzfbDfud2ZN72u8e-Pcs9AG8MhXvnxV3MB1JFbcnJ_raMfwnmiQ6ynH4K418VZCVFdhcY7NsZ97e8osp8YqjSffvbzxuduwTt6yGnPn93gC0_uCoqxD1-lapL2UPDYL6PEBWkLi_tLpvuTeDbga53ZkDJb2ol2Ik7Z5h9WhJ542B78df--FjNjcGG09LVNnOIJYM4ZgfiAdjE12zFc9kKlSTNC4-mzn4g-SOvEoe_Jm0iZLAg7VTihdg9gxoWOjgSHWTPLLc7sFBrQSFjWMHlxBSl5TFpDCnFXu_5Btm17Fuf5YpUWOb4_2w3FsNsi6_2PwRybkAOlaapy3M0ltB2ooKFyKe-7zqwyfuF3bH2OxsJAVSpWshSUXHN5Ec_W4YjHQu26rvbhEcqMJaG0xGpDk-PkjlJYkBOFEQGW77ASWoahGAd5e05_WNUY0tOKvd2ogym8xddLk./download [1115393/1115393] -> \"input.txt\" [1]\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKvPe0jg5V12"
      },
      "source": [
        "Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJth0lkU5X8V",
        "outputId": "0139f9fc-c770-484d-e033-9f9c17325bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_loader = TextLoader('', batch_size, seq_length)\n",
        "vocab_size = data_loader.vocab_size\n",
        "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
        "print (\"Characters:\" ,data_loader.chars)\n",
        "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
        "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading text file\n",
            "vocabulary size: 65\n",
            "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
            "vocab number of 'F': 49\n",
            "Character sequences (first batch): [[49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1\n",
            "   0 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1\n",
            "   4  7]\n",
            " [15 46 10 10 47 32 34 35 31 29 24 10 47  9  6 23 13  2  1  0  8  3  2  0\n",
            "  17  9  2  5  0  5  1  7 38  0  6  5  1  0  9  6  0 11 13  8  4  2  9 19\n",
            "  25 10]\n",
            " [ 0  2  5  4  2  0 15  3 13  8 20  0 42  4  7  6  0  3 18  0 14  1  8 16\n",
            "  10 49  7  3 14  0 18  3  7  2  5  0  2  5  1  0  7  4  8 28  6  0  3 18\n",
            "   0 14]\n",
            " [11  1  8  2  0 28  8  4 27  1  0  9  6  0  2  5  9  6  0  6  4 14  1 46\n",
            "  10 10 35  1 19  3  8 12  0 42 13  6  9 19  9  4  8 24 10 41  4  8 20  0\n",
            "   5  9]\n",
            " [ 6 16  0 21 10 35  5  4 11 11  0 22  7  9  8 20  0 31 14  9 11  9  4  0\n",
            "  18  3  7  2  5 25 10 10 51 26 40 36 21 33 26 24 10 21  0 23  7  4 15  0\n",
            "   8  3]\n",
            " [ 3 13  0  4  7  1  0  2  3  0 12  3  0 14  1  0 22  3  2  5  0  4  0 23\n",
            "   7  1  6  1  8  2  0  4  8 12  0  4 10 12  4  8 20  1  7  3 13  6  0 19\n",
            "   3 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soKGNA2T5lzC"
      },
      "source": [
        "### LSTM Architecture\n",
        "Each LSTM cell has 5 parts:\n",
        "1. Input\n",
        "2. prv_state\n",
        "3. prv_output\n",
        "4. new_state\n",
        "5. new_output\n",
        "\n",
        "\n",
        "- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.\n",
        "- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size\n",
        "- An LSTM keeps two pieces of information as it propagates through time: \n",
        "    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
        "    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
        "\n",
        "\n",
        "#### num_layers = 2 \n",
        "- number of layers in the RNN, is defined by num_layers\n",
        "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt4KCdEL6jfY"
      },
      "source": [
        "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
        "\n",
        "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size\n",
        "representing the batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0IoY7Hc6q_E"
      },
      "source": [
        "### Embedding\n",
        "In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix. The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsunPUyD7ScN"
      },
      "source": [
        "### Feeding a batch of 50 sequence to a RNN:\n",
        "\n",
        "The feeding process for iputs is as following:\n",
        "\n",
        "- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  \n",
        "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
        "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
        "\n",
        "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd1fwyiO7pRR"
      },
      "source": [
        "class LSTMModel():\n",
        "    def __init__(self,sample=False):\n",
        "        rnn_size = 128 # size of RNN hidden state vector\n",
        "        batch_size = 6 # minibatch size, i.e. size of dataset in each epoch\n",
        "        seq_length = 50 # RNN sequence length\n",
        "        num_layers = 2 # number of layers in the RNN\n",
        "        vocab_size = 65\n",
        "        grad_clip = 5.\n",
        "        if sample:\n",
        "            print(\">> sample mode:\")\n",
        "            batch_size = 1\n",
        "            seq_length = 1\n",
        "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
        "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
        "        # model.cell.state_size is (128, 128)\n",
        "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
        "\n",
        "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
        "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
        "        # Initial state of the LSTM memory.\n",
        "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
        "        self.initial_state = self.stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
        "\n",
        "        with tf.variable_scope('rnnlm_class1'):\n",
        "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
        "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
        "            with tf.device(\"/cpu:0\"):\n",
        "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
        "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
        "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
        "                #inputs = tf.split(em, seq_length, 1)\n",
        "                \n",
        "                \n",
        "\n",
        "\n",
        "        # The value of state is updated after processing each batch of chars.\n",
        "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
        "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
        "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "        self.probs = tf.nn.softmax(self.logits)\n",
        "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
        "                [tf.reshape(self.targets, [-1])],\n",
        "                [tf.ones([batch_size * seq_length])],\n",
        "                vocab_size)\n",
        "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
        "        self.final_state = last_state\n",
        "        self.lr = tf.Variable(0.0, trainable=False)\n",
        "        tvars = tf.trainable_variables()\n",
        "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
        "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
        "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
        "    \n",
        "    \n",
        "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
        "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
        "        #print state\n",
        "        for char in prime[:-1]:\n",
        "            x = np.zeros((1, 1))\n",
        "            x[0, 0] = vocab[char]\n",
        "            feed = {self.input_data: x, self.initial_state:state}\n",
        "            [state] = sess.run([self.final_state], feed)\n",
        "\n",
        "        def weighted_pick(weights):\n",
        "            t = np.cumsum(weights)\n",
        "            s = np.sum(weights)\n",
        "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
        "\n",
        "        ret = prime\n",
        "        char = prime[-1]\n",
        "        for n in range(num):\n",
        "            x = np.zeros((1, 1))\n",
        "            x[0, 0] = vocab[char]\n",
        "            feed = {self.input_data: x, self.initial_state:state}\n",
        "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
        "            p = probs[0]\n",
        "\n",
        "            if sampling_type == 0:\n",
        "                sample = np.argmax(p)\n",
        "            elif sampling_type == 2:\n",
        "                if char == ' ':\n",
        "                    sample = weighted_pick(p)\n",
        "                else:\n",
        "                    sample = np.argmax(p)\n",
        "            else: # sampling_type == 1 default:\n",
        "                sample = weighted_pick(p)\n",
        "\n",
        "            pred = chars[sample]\n",
        "            ret += pred\n",
        "            char = pred\n",
        "        return ret"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkhcOepW7qGw"
      },
      "source": [
        "# Creating the LSTM object\n",
        "# Now we create a LSTM model:\n",
        "\n",
        "with tf.variable_scope(\"rnn\"):\n",
        "    model = LSTMModel()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWhubtWk7pQs",
        "outputId": "f3814903-816a-4e82-f3b6-fc5f0a80c26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train usinng LSTMModel class\n",
        "#We can train our model through feeding batches:\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
        "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
        "        data_loader.reset_batch_pointer()\n",
        "        state = sess.run(model.initial_state) # (2x[60x128])\n",
        "        for b in range(data_loader.num_batches): #for each batch\n",
        "            start = time.time()\n",
        "            x, y = data_loader.next_batch()\n",
        "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
        "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
        "            end = time.time()\n",
        "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
        "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
        "        with tf.variable_scope(\"rnn\", reuse=True):\n",
        "            sample_model = LSTMModel(sample=True)\n",
        "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1))\n",
        "            print ('----------------------------------')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3716/18585 (epoch 0), train_loss = 1.668, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The malle ropo sawty then he maid they fairle. Would w\n",
            "----------------------------------\n",
            "7433/18585 (epoch 1), train_loss = 1.564, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The spitte\n",
            "But have begants out, and mine now\n",
            "so, bust\n",
            "----------------------------------\n",
            "11150/18585 (epoch 2), train_loss = 1.514, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The us my brown's\n",
            "Are; and as you you uncquasiors of y\n",
            "----------------------------------\n",
            "14867/18585 (epoch 3), train_loss = 1.538, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The lacious deceas, We then near? where, merrain whose\n",
            "----------------------------------\n",
            "18584/18585 (epoch 4), train_loss = 1.502, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The sluck stabe of Chiec.\n",
            "Why, then a pary; he part.\n",
            "\n",
            "\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}