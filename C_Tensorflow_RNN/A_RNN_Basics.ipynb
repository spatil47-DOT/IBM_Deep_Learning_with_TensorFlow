{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A_RNN_Basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEa6cURb3OJj"
      },
      "source": [
        "<br>\n",
        "<h2>Table of Contents</h2>\n",
        "<ol>\n",
        "    <li><a href=\"#intro\">Introduction</a></li>\n",
        "    <li><a href=\"#long_short_term_memory_model\">Long Short-Term Memory Model</a></li>\n",
        "    <li><a href=\"#ltsm\">LTSM</a></li>\n",
        "    <li><a href=\"#stacked_ltsm\">Stacked LTSM</a></li>\n",
        "</ol>\n",
        "<p></p>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHeIgZKz3n1h"
      },
      "source": [
        "<a id=\"intro\"><a/> \n",
        "<h2>Introduction</h2>\n",
        "\n",
        "Recurrent Neural Networks are Deep Learning models with simple structures and a feedback mechanism built-in, or in different words, the output of a layer is added to the next input and fed back to the same layer.\n",
        "\n",
        "The Recurrent Neural Network is a specialized type of Neural Network that solves the issue of **maintaining context for Sequential data** -- such as Weather data, Stocks, Genes, etc. At each iterative step, the processing unit takes in an input and the current state of the network, and produces an output and a new state that is <b> re-fed into the network</b>.\n",
        "\n",
        "<h3>Representation of a Recurrent Neural Network</h3>\n",
        "\n",
        "However, <b> this model has some problems</b>. It's very computationally expensive to maintain the state for a large amount of units, even more so over a long amount of time. Additionally, Recurrent Networks are very sensitive to changes in their parameters. As such, they are prone to different problems with their Gradient Descent optimizer -- they either grow exponentially (Exploding Gradient) or drop down to near zero and stabilize (Vanishing Gradient), both problems that greatly harm a model's learning capability.\n",
        "\n",
        "To solve these problems, Hochreiter and Schmidhuber published a paper in 1997 describing a way to keep information over long periods of time and additionally solve the oversensitivity to parameter changes, i.e., make backpropagating through the Recurrent Networks more viable. This proposed method is called Long Short-Term Memory (LSTM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5emXLGo4Y6a"
      },
      "source": [
        "<a id=\"long_short_term_memory_model\"></a>\n",
        "<h2>Long Short-Term Memory Model</h2>\n",
        "\n",
        "The Long Short-Term Memory, as it was called, was an abstraction of how computer memory works. It is \"bundled\" with whatever processing unit is implemented in the Recurrent Network, although outside of its flow, and is responsible for keeping, reading, and outputting information for the model. The way it works is simple: you have a linear unit, which is the information cell itself, surrounded by three logistic gates responsible for maintaining the data. One gate is for inputting data into the information cell, one is for outputting data from the input cell, and the last one is to keep or forget data depending on the needs of the network.\n",
        "\n",
        "Thanks to that, it not only solves the problem of keeping states, because the network can choose to forget data whenever information is not needed, it also solves the gradient problems, since the Logistic Gates have a very nice derivative.\n",
        "\n",
        "<h3>Long Short-Term Memory Architecture</h3>\n",
        "\n",
        "The Long Short-Term Memory is composed of a linear unit surrounded by three logistic gates. The name for these gates vary from place to place, but the most usual names for them are:\n",
        "\n",
        "<ul>\n",
        "    <li>the \"Input\" or \"Write\" Gate, which handles the writing of data into the information cell</li>\n",
        "    <li>the \"Output\" or \"Read\" Gate, which handles the sending of data back onto the Recurrent Network</li>\n",
        "    <li>the \"Keep\" or \"Forget\" Gate, which handles the maintaining and modification of the data stored in the information cell</li>\n",
        "</ul>\n",
        "As mentioned many times, all three gates are logistic. The reason for this is because it is very easy to backpropagate through them, and as such, it is possible for the model to learn exactly _how_ it is supposed to use this structure. This is one of the reasons for which LSTM is a very strong structure. Additionally, this solves the gradient problems by being able to manipulate values through the gates themselves -- by passing the inputs and outputs through the gates, we have now a easily derivable function modifying our inputs.\n",
        "\n",
        "In regards to the problem of storing many states over a long period of time, LSTM handles this perfectly by only keeping whatever information is necessary and forgetting it whenever it is not needed anymore. Therefore, LSTMs are a very elegant solution to both problems.\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr2UjNY748aH"
      },
      "source": [
        "<a id=\"ltsm\"></a>\n",
        "<h2>LSTM</h2>\n",
        "Lets first create a tiny LSTM network sample to understand the architecture of LSTM networks.\n",
        "\n",
        "We need to import the necessary modules for our code. We need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>, obviously. Additionally, we can import directly the <b><code>tensorflow.contrib.rnn</code></b> model, which includes the function for building RNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nC4-6kx7mT1",
        "outputId": "867dea6b-d86d-4c44-ce07-545322ecdfb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        }
      },
      "source": [
        "# install tensorflow=1.8 and restart Runtime before executiing subsequent cells\n",
        "!pip install tensorflow==1.8.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1MB 82kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (3.12.4)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.18.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.33.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0) (50.3.2)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.1MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.4.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=5106ff0cdd75f3a44b15280cb6d9b127aeaf3459b75094cf6010a3baf7042d50\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUahCzZv7qwo"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "sess = tf.Session()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMpRrkfp5GT7"
      },
      "source": [
        "We want to create a network that has only one LSTM cell. We have to pass 2 elements to LSTM, the <b>prv_output</b> and <b>prv_state</b>, so called, <b>h</b> and <b>c</b>. Therefore, we initialize a state vector, <b>state</b>.  Here, <b>state</b> is a tuple with 2 elements, each one is of size [1 x 4], one for passing prv_output to next time step, and another for passing the prv_state to next time stamp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgzKoQ7U5I1y",
        "outputId": "a615adab-c4c0-4a4c-a922-284e98fb18c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "LSTM_CELL_SIZE = 4  # output size (dimension), which is same as hidden size in the cell\n",
        "\n",
        "lstm_cell = tf.contrib.rnn.BasicLSTMCell(LSTM_CELL_SIZE, state_is_tuple=True)\n",
        "state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n",
        "state"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'zeros_1:0' shape=(1, 4) dtype=float32>,\n",
              " <tf.Tensor 'zeros_1:0' shape=(1, 4) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-AeP2bm5am0",
        "outputId": "6ceb0af5-d7c9-4c6e-fa60-212732e5f1cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_input = tf.constant([[3,2,2,2,2,2]],dtype=tf.float32)\n",
        "print (sess.run(sample_input))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3. 2. 2. 2. 2. 2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8wdxcPb5loZ",
        "outputId": "0440a54a-20fb-4f3d-fe38-62ee42413b9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now, we can pass the input to lstm_cell, and check the new state:\n",
        "with tf.variable_scope(\"LSTM_sample1\"):\n",
        "    output, state_new = lstm_cell(sample_input, state)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "print (sess.run(state_new))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMStateTuple(c=array([[ 0.0375159 , -0.02935171, -0.8336087 , -0.04389074]],\n",
            "      dtype=float32), h=array([[ 0.0213045 , -0.02564878, -0.46928686, -0.03299242]],\n",
            "      dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNTixp4g5uOj",
        "outputId": "68d07680-a346-48ca-97f8-1bd923b0ebd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (sess.run(output))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.0213045  -0.02564878 -0.46928686 -0.03299242]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqlXgIh75126"
      },
      "source": [
        "<hr>\n",
        "<a id=\"stacked_ltsm\"></a>\n",
        "<h2>Stacked LSTM</h2>\n",
        "What about if we want to have a RNN with stacked LSTM? For example, a 2-layer LSTM. In this case, the output of the first layer will become the input of the second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrZJ61aX558j"
      },
      "source": [
        "# Lets start with a new session:\n",
        "sess = tf.Session()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eW1JBc16KeW"
      },
      "source": [
        "input_dim = 6"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy86jzhN6W-9"
      },
      "source": [
        "# Lets create the stacked LSTM cell:\n",
        "cells = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_xhMK2F6d_C"
      },
      "source": [
        "# Creating the first layer LTSM cell.\n",
        "LSTM_CELL_SIZE_1 = 4 #4 hidden nodes\n",
        "cell1 = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE_1)\n",
        "cells.append(cell1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AoxxmOD6nzA"
      },
      "source": [
        "# Creating the second layer LTSM cell.\n",
        "LSTM_CELL_SIZE_2 = 5 #5 hidden nodes\n",
        "cell2 = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE_2)\n",
        "cells.append(cell2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZeTrBQq6vqu"
      },
      "source": [
        "# To create a multi-layer LTSM we use the <b>tf.contrib.rnnMultiRNNCell</b> function, it takes in multiple single layer LTSM cells to create a multilayer stacked LTSM model.\n",
        "stacked_lstm = tf.contrib.rnn.MultiRNNCell(cells)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29EiiRXx63hA"
      },
      "source": [
        "# Now we can create the RNN from <b>stacked_lstm</b>:\n",
        "# Batch size x time steps x features.\n",
        "data = tf.placeholder(tf.float32, [None, None, input_dim])\n",
        "output, state = tf.nn.dynamic_rnn(stacked_lstm, data, dtype=tf.float32)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4x4SnXY7D6D"
      },
      "source": [
        "Lets say the input sequence length is 3, and the dimensionality of the inputs is 6. The input should be a Tensor of shape: [batch_size, max_time, dimension], in our case it would be (2, 3, 6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIC5Tq637CJ6",
        "outputId": "2d3ae2d9-dca5-4e74-962d-be8d2bc0f969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Batch size x time steps x features.\n",
        "sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\n",
        "sample_input"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1, 2, 3, 4, 3, 2], [1, 2, 1, 1, 1, 2], [1, 2, 2, 2, 2, 2]],\n",
              " [[1, 2, 3, 4, 3, 2], [3, 2, 2, 1, 1, 2], [0, 0, 0, 0, 3, 2]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K6yylOb7KDx",
        "outputId": "dfc117df-d837-45a2-bf43-c2f68bcc5101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we can now send our input to network, and check the output:\n",
        "output"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'rnn/transpose_1:0' shape=(?, ?, 5) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMYqwFPV7Rcp",
        "outputId": "b8d4678c-47f2-47ed-f35b-d69e2009d3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(output, feed_dict={data: sample_input})"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.01748371, -0.04756487,  0.01044648, -0.03325069,\n",
              "          0.00728339],\n",
              "        [ 0.02900894, -0.08259416,  0.03520614, -0.047414  ,\n",
              "          0.00997212],\n",
              "        [ 0.05008117, -0.12135594,  0.05984164, -0.06363051,\n",
              "          0.01179782]],\n",
              "\n",
              "       [[ 0.01748371, -0.04756487,  0.01044648, -0.03325069,\n",
              "          0.00728339],\n",
              "        [ 0.03516915, -0.06727328,  0.01352777, -0.06188452,\n",
              "          0.00592734],\n",
              "        [ 0.056821  , -0.07576431,  0.02042386, -0.059009  ,\n",
              "          0.00361702]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA_ewByL7WWf"
      },
      "source": [
        "As you see, the output is of shape (2, 3, 5), which corresponds to our 2 batches, 3 elements in our sequence, and the dimensionality of the output which is 5.\n"
      ]
    }
  ]
}